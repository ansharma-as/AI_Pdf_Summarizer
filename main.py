from langchain_community.llms import LlamaCpp
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

import os
import time

MODEL_PATH = "./mistral-7b-instruct-v0.1.Q4_K_M.gguf"
PDF_PATH = "./document.pdf"

llm = LlamaCpp(
    model_path=MODEL_PATH,
    temperature=0.1,      
    max_tokens=256,      
    top_p=0.9,
    n_ctx=1024,          
    n_batch=128,        
    f16_kv=True,
    verbose=False,
    stop=["\n\n", "Question:", "Context:"],  
)

print("ğŸ”„ Loading PDF...")
loader = PyPDFLoader(PDF_PATH)
documents = loader.load()

print("âœ‚ï¸ Splitting text...")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

print("ğŸ§  Creating embeddings...")
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embedding)

print(f"âœ… Ready! Processed {len(documents)} pages into {len(docs)} chunks")
print("ğŸ¤– Ask questions about the PDF. Type 'exit' to quit.\n")

while True:
    query = input("â“ Your question: ")
    if query.lower() == "exit":
        break

    try:
        print("ğŸ” Searching for relevant content...")
        start_time = time.time()
        
        relevant_docs = vectorstore.similarity_search(query, k=2)  
        
        if not relevant_docs:
            print("âŒ No relevant content found.")
            continue
        
        pages = [doc.metadata.get('page', 'Unknown') for doc in relevant_docs]
        print(f"ğŸ“„ Found content from pages: {pages}")
        
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        context = context[:800]  
        print(f"ğŸ“ Context length: {len(context)} characters")
        print(f"ğŸ“ Preview: {context[:150]}...")
        
        prompt = f"""Based on the document text below, answer the question.

Document text:
{context}

Question: {query}

Answer (be concise and specific):"""

        print(f"ğŸ¤” Generating answer (prompt length: {len(prompt)})...")
        
        gen_start = time.time()
        response = llm.invoke(prompt)
        gen_time = time.time() - gen_start
        
        print(f"âœ… Generated in {gen_time:.1f} seconds")
        print("\nğŸ¤– Answer:")
        print("-" * 40)
        print(response.strip())
        print("-" * 40)
        print(f"ğŸ“š Sources: Pages {pages}\n")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Generation interrupted by user")
        continue
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        print("ğŸ’¡ Try a simpler question or restart the program\n")